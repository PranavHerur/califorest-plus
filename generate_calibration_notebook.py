import json

notebook_content = {
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Analysis of Random Forest Calibration Techniques\n\n",
                "This notebook presents the results of comparing different calibration techniques for Random Forest models, following the analysis plan.",
            ],
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Data Loading\n\n",
                "Import necessary libraries and load the experimental results. \n\n",
                "**Note:** Replace the data generation part below with loading your actual results, likely from a CSV or other data format. The DataFrame should contain columns like 'Technique', 'Dataset/Fold', 'Brier', 'ECE', 'LogLoss', 'Accuracy', 'AUC_ROC', etc.",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from scipy import stats\n",
                "from sklearn.calibration import calibration_curve\n",
                "from sklearn.metrics import brier_score_loss, log_loss, accuracy_score, roc_auc_score\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid') # Use a nice style\n",
                "\n",
                "# --- Placeholder Data Generation --- \n",
                "# Replace this section with loading your actual data\n",
                "np.random.seed(42)\n",
                "techniques = ['Uncalibrated', 'Platt Scaling', 'Isotonic Regression', 'Beta Calibration']\n",
                "datasets = [f'Dataset_{i+1}' for i in range(10)] # Simulate 10 datasets/folds\n",
                "data = []\n",
                "\n",
                "for tech in techniques:\n",
                "    for ds in datasets:\n",
                "        # Simulate metrics (adjust ranges based on expected performance)\n",
                "        if tech == 'Uncalibrated':\n",
                "            brier = np.random.uniform(0.15, 0.25)\n",
                "            ece = np.random.uniform(0.05, 0.15)\n",
                "            logloss = np.random.uniform(0.4, 0.6)\n",
                "            acc = np.random.uniform(0.75, 0.85)\n",
                "            auc = np.random.uniform(0.80, 0.90)\n",
                "        elif tech == 'Platt Scaling':\n",
                "            brier = np.random.uniform(0.10, 0.20)\n",
                "            ece = np.random.uniform(0.02, 0.08)\n",
                "            logloss = np.random.uniform(0.3, 0.5)\n",
                "            acc = np.random.uniform(0.75, 0.86) # Small potential change\n",
                "            auc = np.random.uniform(0.80, 0.91)\n",
                "        elif tech == 'Isotonic Regression':\n",
                "            brier = np.random.uniform(0.08, 0.18)\n",
                "            ece = np.random.uniform(0.01, 0.06)\n",
                "            logloss = np.random.uniform(0.25, 0.45)\n",
                "            acc = np.random.uniform(0.74, 0.85)\n",
                "            auc = np.random.uniform(0.79, 0.90)\n",
                "        else: # Beta Calibration\n",
                "            brier = np.random.uniform(0.09, 0.19)\n",
                "            ece = np.random.uniform(0.015, 0.07)\n",
                "            logloss = np.random.uniform(0.28, 0.48)\n",
                "            acc = np.random.uniform(0.75, 0.85)\n",
                "            auc = np.random.uniform(0.80, 0.90)\n",
                "            \n",
                "        data.append({'Technique': tech, 'Dataset': ds, \n",
                "                     'Brier': brier, 'ECE': ece, 'LogLoss': logloss,\n",
                "                     'Accuracy': acc, 'AUC_ROC': auc})\n",
                "\n",
                "results_df = pd.DataFrame(data)\n",
                "# --- End Placeholder Data --- \n",
                "\n",
                'print("Loaded Results (first 5 rows):")\n',
                "print(results_df.head())",
            ],
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Overall Metric Comparison\n\n",
                "Calculate and display the average performance metrics across all datasets/folds for each calibration technique.",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "average_metrics = results_df.groupby('Technique').mean(numeric_only=True)\n",
                'print("Average Metrics Across All Datasets:")\n',
                "print(average_metrics)\n",
                "\n",
                "# Optional: Plot average Brier Score and ECE\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
                "average_metrics['Brier'].sort_values().plot(kind='bar', ax=axes[0], title='Average Brier Score (Lower is Better)')\n",
                "axes[0].set_ylabel('Brier Score')\n",
                "axes[0].tick_params(axis='x', rotation=45)\n",
                "\n",
                "average_metrics['ECE'].sort_values().plot(kind='bar', ax=axes[1], title='Average ECE (Lower is Better)')\n",
                "axes[1].set_ylabel('Expected Calibration Error (ECE)')\n",
                "axes[1].tick_params(axis='x', rotation=45)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()",
            ],
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Reliability Diagrams (Calibration Curves)\n\n",
                "Visualize the calibration performance using reliability diagrams. \n\n",
                "**Note:** This requires the true labels (`y_true`) and the predicted probabilities (`y_prob`) for each technique *before* aggregation into metrics. You will need to load or generate this detailed data. The code below simulates this for demonstration.",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Placeholder Probability Generation --- \n",
                "# Replace this with loading your actual y_true and y_prob arrays for each technique\n",
                "# For demonstration, we generate mock probabilities for one 'representative' dataset\n",
                "n_samples = 1000\n",
                "y_true_mock = np.random.randint(0, 2, n_samples)\n",
                "\n",
                "# Generate probabilities that roughly match the expected calibration behaviors\n",
                "prob_uncalibrated = np.clip(np.random.normal(0.5 + (y_true_mock - 0.5) * 0.6, 0.3, n_samples), 0.01, 0.99)\n",
                "prob_platt = 1 / (1 + np.exp(-(prob_uncalibrated * 1.5 - 0.75))) # Simple sigmoid scaling\n",
                "prob_isotonic = np.sort(prob_uncalibrated) # Mock isotonic (just sorting for demo)\n",
                "prob_beta = np.clip(prob_uncalibrated * 0.8 + 0.1, 0.01, 0.99) # Mock beta \n",
                "\n",
                "mock_probs = {\n",
                "    'Uncalibrated': prob_uncalibrated,\n",
                "    'Platt Scaling': prob_platt,\n",
                "    'Isotonic Regression': prob_isotonic,\n",
                "    'Beta Calibration': prob_beta\n",
                "}\n",
                "# --- End Placeholder --- \n",
                "\n",
                "plt.figure(figsize=(8, 8))\n",
                "ax = plt.subplot(111)\n",
                "plt.plot([0, 1], [0, 1], 'k:', label='Perfectly calibrated')\n",
                "\n",
                "for tech, y_prob in mock_probs.items():\n",
                "    # Ensure y_prob and y_true_mock are available here from your data\n",
                "    fraction_of_positives, mean_predicted_value = calibration_curve(y_true_mock, y_prob, n_bins=10, strategy='uniform')\n",
                "    plt.plot(mean_predicted_value, fraction_of_positives, 's-', label=f'{tech}')\n",
                "\n",
                "ax.set_xlabel('Mean Predicted Probability')\n",
                "ax.set_ylabel('Fraction of Positives')\n",
                "ax.set_ylim([-0.05, 1.05])\n",
                "ax.set_title('Reliability Diagram (Calibration Curve)')\n",
                "ax.legend(loc='lower right')\n",
                "plt.grid(True)\n",
                "plt.show()",
            ],
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Distribution of Metrics\n\n",
                "Use box plots to visualize the distribution of key calibration metrics (e.g., Brier Score, ECE) across different datasets or cross-validation folds.",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
                "\n",
                "sns.boxplot(data=results_df, x='Technique', y='Brier', ax=axes[0])\n",
                "axes[0].set_title('Distribution of Brier Score Across Datasets')\n",
                "axes[0].set_ylabel('Brier Score (Lower is Better)')\n",
                "axes[0].tick_params(axis='x', rotation=45)\n",
                "\n",
                "sns.boxplot(data=results_df, x='Technique', y='ECE', ax=axes[1])\n",
                "axes[1].set_title('Distribution of ECE Across Datasets')\n",
                "axes[1].set_ylabel('ECE (Lower is Better)')\n",
                "axes[1].tick_params(axis='x', rotation=45)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()",
            ],
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Statistical Significance Testing\n\n",
                "Perform statistical tests to determine if the differences in performance (e.g., Brier Score, ECE) between techniques are statistically significant. We use the Wilcoxon signed-rank test as it's suitable for paired non-parametric data (comparing techniques on the same datasets/folds).",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "baseline = 'Uncalibrated'\n",
                "metrics_to_test = ['Brier', 'ECE', 'LogLoss'] # Add other relevant metrics\n",
                "alpha = 0.05 # Significance level\n",
                "\n",
                "print(f\"Statistical Tests (Wilcoxon Signed-Rank) comparing to '{baseline}':\\n\")\n",
                "\n",
                "for metric in metrics_to_test:\n",
                '    print(f"--- Testing Metric: {metric} ---")\n',
                "    baseline_data = results_df[results_df['Technique'] == baseline][metric].values\n",
                "    for tech in techniques:\n",
                "        if tech == baseline: continue\n",
                "        \n",
                "        tech_data = results_df[results_df['Technique'] == tech][metric].values\n",
                "        \n",
                "        # Ensure paired data (same number of datasets/folds)\n",
                "        if len(baseline_data) == len(tech_data):\n",
                "            try:\n",
                "                 # Alternative: 'less' if lower score is better, 'greater' if higher is better\n",
                "                statistic, p_value = stats.wilcoxon(baseline_data, tech_data, alternative='two-sided') \n",
                "                print(f\"  {tech} vs {baseline}: p-value = {p_value:.4f}\", end='')\n",
                "                if p_value < alpha:\n",
                "                    # Check direction for Brier/ECE/LogLoss (lower is better)\n",
                "                    if np.mean(tech_data) < np.mean(baseline_data):\n",
                '                         print(f" (Significant improvement over {baseline})")\n',
                "                    else:\n",
                '                         print(f" (Significant difference, but not improvement over {baseline})")\n',
                "                else:\n",
                '                    print(" (Not Significant)")\n',
                "            except ValueError as e:\n",
                '                 print(f"  {tech} vs {baseline}: Could not perform test (e.g., identical values) - {e}")\n',
                "        else:\n",
                '            print(f"  {tech} vs {baseline}: Data length mismatch, cannot perform paired test.")\n',
                "    print()\n",
                "\n",
                "# Optional: Consider Friedman test + post-hoc (Nemenyi) if comparing all methods simultaneously\n",
                "# from scipy.stats import friedmanchisquare\n",
                "# Add code here if needed",
            ],
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Key Findings Summary\n\n",
                "Summarize the main conclusions based on the tables, plots, and statistical tests.\n\n",
                "*   **Overall Performance:** Which technique(s) consistently showed the best calibration metrics (lowest Brier, ECE, LogLoss) on average? (Refer to Table/Bar Chart in Section 2).\n",
                "*   **Calibration Quality:** How did the reliability diagrams differ? Which techniques appeared closest to the ideal diagonal line? (Refer to Figure in Section 3).\n",
                "*   **Consistency:** Which techniques showed the least variance in performance across datasets (narrowest box plots)? (Refer to Figure in Section 4).\n",
                "*   **Statistical Significance:** Were the improvements offered by the calibration techniques statistically significant compared to the uncalibrated baseline? Did significant differences exist between the calibration techniques themselves? (Refer to p-values in Section 5).\n",
                "*   **Impact on Discrimination:** Did calibration significantly affect discriminative performance (Accuracy, AUC_ROC)? (Refer to Table in Section 2 and potentially statistical tests on these metrics).\n",
                "*   **Example Finding (based on mock data):** *Isotonic Regression and Beta Calibration significantly improved Brier score and ECE compared to the uncalibrated baseline (p < 0.05), with Isotonic Regression showing slightly lower median ECE but higher variance across datasets. Platt Scaling also offered significant improvements, though generally less pronounced than Isotonic or Beta. Discriminative performance (Accuracy, AUC_ROC) was not significantly altered by any calibration method.*",
            ],
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Discussion and Limitations\n\n",
                "Discuss the implications of the findings and acknowledge limitations.\n\n",
                "*   **Interpretation:** What do these results mean for practitioners wanting to use calibrated Random Forest models?\n",
                "*   **Context:** Are there specific scenarios (e.g., based on dataset characteristics not captured here, like imbalance) where one technique might be preferred over others?\n",
                "*   **Limitations of this Study:** \n",
                "    *   Mention the specific datasets used and that results might vary on others.\n",
                "    *   Mention the range of hyperparameters explored for RF and the calibration methods.\n",
                "    *   Acknowledge limitations of specific metrics (e.g., ECE binning choices).\n",
                "*   **Future Work:** Suggest potential follow-up research (e.g., testing on more diverse datasets, exploring different hyperparameter settings, investigating computational costs).",
            ],
        },
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3",
        },
        "language_info": {
            "codemirror_mode": {"name": "ipython", "version": 3},
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x.y",  # Replace with your actual python version
        },
    },
    "nbformat": 4,
    "nbformat_minor": 2,
}

# Create the notebook file
file_path = "rf_calibration_results.ipynb"
with open(file_path, "w") as f:
    json.dump(notebook_content, f, indent=2)

print(f"Jupyter notebook '{file_path}' created successfully.")
print("Please open it and replace placeholder data/code with your actual results.")
